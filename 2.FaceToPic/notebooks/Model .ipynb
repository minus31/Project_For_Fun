{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T08:57:03.200735Z",
     "start_time": "2018-08-14T08:57:01.988480Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/MAC/.pyenv/versions/anaconda3-5.0.1/envs/python_ana/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e1fa03f3e538>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'module'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "from module import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-14T09:01:13.561703Z",
     "start_time": "2018-08-14T09:01:13.558033Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-8c7e43f5f5b3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-8c7e43f5f5b3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class Args()\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Args():\n",
    "    def __init__(self, batch_size, image_size, input_dim, \n",
    "                 output_dim, L1_lambda, tfrecord_dir, \n",
    "                 ngf, ndf, output_nc, max_size, phase='train'):\n",
    "        self.batch_size  = batch_size \n",
    "        self.image_size = image_size\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.L1_lambda = L1_lambda\n",
    "        self.tfrecord_dir = tfrecord_dir\n",
    "        self.ngf = ngf \n",
    "        self.ndf = ndf \n",
    "        self.output_nc = output_nc\n",
    "        self.phase = phase\n",
    "        self.max_size = max_size\n",
    "        \n",
    "\n",
    "args = Args(batch_size=34, \n",
    "            image_size=300, \n",
    "            input_dim=1, \n",
    "            output_dim=1, \n",
    "            L1_lambda, \n",
    "            tfrecord_dir,\n",
    "            ngf,\n",
    "            ndf, \n",
    "            output_nc,\n",
    "            phase,\n",
    "            max_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGan():\n",
    "    \n",
    "    def __init__(self, sess, args, loss_gan=True, is_training=True):\n",
    "        self.sess = sess                \n",
    "        self.batch_size = args.batch_size    \n",
    "        self.image_size = args.image_size    \n",
    "        self.input_dim = args.input_dim      \n",
    "        self.output_dim = args.output_dim    \n",
    "        self.L1_lambda = args.L1_lambda      \n",
    "        self.tfrecord_dir = args.tfrecord_dir\n",
    "        self.is_training = is_training  \n",
    "        \n",
    "        self.discriminator = discriminator \n",
    "        self.generator = generator_densenet\n",
    "        \n",
    "        if loss_gan == True:\n",
    "            self.criterionGAN = abs_criterion\n",
    "        else :\n",
    "            self.criterionGAN = sce_criterion \n",
    "            \n",
    "        OPTIONS = namedtuple(\"OPTIONS\", \n",
    "                            \"batch_size image_size gf_dim dfdim output_c_dim is_training\")\n",
    "        \n",
    "        self.options = OPTIONS._make((args.batch_size, args.fine_size,\n",
    "                                      args.ngf, args.ndf, args.output_nc,\n",
    "                                      args.phase == 'train'))\n",
    "        \n",
    "        self._build_model()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.pool = ImagePool(args.max_size)\n",
    "        \n",
    "    def _bulid_model(self):\n",
    "        self.real_data = tf.placeholder(tf.float32,\n",
    "                                       [None, self.image_size, self.image_size,\n",
    "                                       self.input_c_dim + self.output_c_dim],\n",
    "                                       name='real_A_and_B_images')\n",
    "        \n",
    "        self.real_A = self.real_data[:, :, :, :self.input_c_dim]\n",
    "        self.real_B = self.real_data[:, :, :, self.input_c_dim:self.input_c_dim + self.output_c_dim]\n",
    "        \n",
    "        self.fake_B = self.generator(self.real_A, reuse=False, name=\"generatorAtoB\")\n",
    "        self.fake_A_ = self.generator(self.fake_B, reuse=False, name=\"generatorBtoA\")\n",
    "        self.fake_A = self.generator(self.real_B, reuse=True, name=\"generatorBtoA\")\n",
    "        self.fake_B_ = self.generator(self.fake_A, reuse=True, name=\"generatorAtoB\")\n",
    "        \n",
    "        \n",
    "        self.DB_fake = self.discriminator(self.fake_B, reuse=False, name=\"discriminatorB\")\n",
    "        self.DA_fake = self.discriminator(self.fake_A, reuse=False, name=\"discriminatorA\")\n",
    "        \n",
    "        \n",
    "        self.gan_loss_a2b = self.criterionGAN(self.DB_fake, tf.ones_like(self.DB_fake)) \\\n",
    "                            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\\n",
    "                            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n",
    "        \n",
    "        self.gan_loss_b2a = self.criterionGAN(self.DA_fake, tf.ones_like(self.DA_fake)) \\ \n",
    "                            + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\ \n",
    "                            + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n",
    "        \n",
    "        self.g_loss = self.criterionGAN(self.DA_fake, tf.ones_like(self.DA_fake)) \\ \n",
    "                      + self.criterionGAN(self.DB_fake, tf.ones_like(self.DB_fake)) \\ \n",
    "                      + self.L1_lambda * abs_criterion(self.real_A, self.fake_A_) \\ \n",
    "                      + self.L1_lambda * abs_criterion(self.real_B, self.fake_B_)\n",
    "        \n",
    "        self.fake_A_sample = tf.placeholder(tf.float32, \n",
    "                                           [None, self.image_size, self.image_size, self.input_c_dim],\n",
    "                                           name='fake_A_sample')\n",
    "        self.fake_B_sample = tf.placeholder(tf.float32, \n",
    "                                           [None, self.image_size, self.image_size, self.output_c_dim],\n",
    "                                           name=\"fake_B_sample\")\n",
    "        \n",
    "        self.DB_real = self.discriminator(self.real_B, reuse=True, name=\"discriminatorB\")\n",
    "        self.DA_real = self.discriminator(self.real_A, reuse=True, name=\"discriminatorA\")\n",
    "        self.DB_fake_sample = self.discriminator(self.fake_B_sample, reuse=True, name='discriminatorB')\n",
    "        self.DA_fake_sample = self.discriminator(self.fake_A_sample, reuse=True, name=\"discriminatorA\")\n",
    "        \n",
    "        self.db_loss_real = self.criterionGAN(self.DB_real, tf.ones_like(self.DB_real))\n",
    "        self.db_loss_fake = self.criterionGAN(self.DB_fake_sample, tf.zeros_like(self.DB_fake_sample))\n",
    "        \n",
    "        self.db_loss = (self.db_loss_real + self.db_loss_fake) / 2 \n",
    "        self.da_loss_real = self.criterionGAN(self.DA_real, tf.ones_like(self.DA_real))\n",
    "        self.da_loss_fake = self.criterionGAN(self.DA_fake_sample, tf.zeros_like(self.DA_fake_sample))\n",
    "        self.da_loss = (self.da_loss_real + self.da_loss_fake) / 2\n",
    "        self.d_loss = self.da_loss + self.db_loss\n",
    "        \n",
    "        self.g_loss_a2b_sum = tf.summary.scalar('g_loss_a2b', self.g_loss_a2b)\n",
    "        self.g_loss_b2a_sum = tf.summary.scalar(\"g_loss_b2a\", self.g_loss_b2a)\n",
    "        self.g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
    "        \n",
    "         self.g_sum = tf.summary.merge([self.g_loss_a2b_sum, self.g_loss_b2a_sum, self.g_loss_sum])\n",
    "        self.db_loss_sum = tf.summary.scalar(\"db_loss\", self.db_loss)\n",
    "        self.da_loss_sum = tf.summary.scalar(\"da_loss\", self.da_loss)\n",
    "        self.d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
    "        self.db_loss_real_sum = tf.summary.scalar(\"db_loss_real\", self.db_loss_real)\n",
    "        self.db_loss_fake_sum = tf.summary.scalar(\"db_loss_fake\", self.db_loss_fake)\n",
    "        self.da_loss_real_sum = tf.summary.scalar(\"da_loss_real\", self.da_loss_real)\n",
    "        self.da_loss_fake_sum = tf.summary.scalar(\"da_loss_fake\", self.da_loss_fake)\n",
    "        self.d_sum = tf.summary.merge(\n",
    "            [self.da_loss_sum, self.da_loss_real_sum, self.da_loss_fake_sum,\n",
    "             self.db_loss_sum, self.db_loss_real_sum, self.db_loss_fake_sum,\n",
    "             self.d_loss_sum]\n",
    "        )\n",
    "        \n",
    "        self.test_A = tf.placeholder(tf.float32, \n",
    "                                    [None, self.image_size, self.image_size, self.input_c_dim],\n",
    "                                     name=\"test_A\")\n",
    "        \n",
    "        self.test_B = tf.placeholder(tf.float32, \n",
    "                                    [None, self.image_size, self.image_size, self.output_c_dim], name=\"test_B\")\n",
    "        \n",
    "        self.test_B = self.generator(self.test_A, reuse=True, name='generator_A2B')\n",
    "        self.test_A = self.generator(self.test_B, reuse=True, name=\"generator_B2A\")\n",
    "        \n",
    "        t_vars = tf.trainable_variables()\n",
    "        self.d_vars = [var for var in t_vars if \"discriminator\" in var.name]\n",
    "        self.g_vars = [var for var in t_vars if \"generator\" in var.name]\n",
    "        \n",
    "        for var in t_vars:\n",
    "            print(var_name)\n",
    "            \n",
    "            \n",
    "    def train(self, args):\n",
    "        \n",
    "        self.lr = tf.placeholder(tf.float32, None, name=\"learning_rate\")\n",
    "        self.d_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1).minimize(self.d_loss, var_list=self.d_vars)\n",
    "        self.g_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1).minimize(self.g_loss, var_list=self.g_vars)\n",
    "        \n",
    "        \n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self.sees.run(init_op)\n",
    "        \n",
    "        self.writer = tf.summary.FileWriter(\"./logs\", self.sess.graph)\n",
    "        \n",
    "        counter = 1 \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if args.continue_train:\n",
    "            if self.load(args.checkpoint_dir):\n",
    "                print(' [*] Load success')\n",
    "                \n",
    "            else :\n",
    "                print(\" [!] Load failed...\")\n",
    "                \n",
    "        for epoch in range(args.epoch):\n",
    "            dataA = \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def train(self, args):\n",
    "        \"\"\"Train cyclegan\"\"\"\n",
    "        self.lr = tf.placeholder(tf.float32, None, name='learning_rate')\n",
    "        self.d_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \\\n",
    "            .minimize(self.d_loss, var_list=self.d_vars)\n",
    "        self.g_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \\\n",
    "            .minimize(self.g_loss, var_list=self.g_vars)\n",
    "\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self.sess.run(init_op)\n",
    "        self.writer = tf.summary.FileWriter(\"./logs\", self.sess.graph)\n",
    "\n",
    "        counter = 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        if args.continue_train:\n",
    "            if self.load(args.checkpoint_dir):\n",
    "                print(\" [*] Load SUCCESS\")\n",
    "            else:\n",
    "                print(\" [!] Load failed...\")\n",
    "\n",
    "        for epoch in range(args.epoch):\n",
    "            dataA = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/trainA'))\n",
    "            dataB = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/trainB'))\n",
    "            np.random.shuffle(dataA)\n",
    "            np.random.shuffle(dataB)\n",
    "            batch_idxs = min(min(len(dataA), len(dataB)), args.train_size) // self.batch_size\n",
    "            lr = args.lr if epoch < args.epoch_step else args.lr*(args.epoch-epoch)/(args.epoch-args.epoch_step)\n",
    "\n",
    "            for idx in range(0, batch_idxs):\n",
    "                batch_files = list(zip(dataA[idx * self.batch_size:(idx + 1) * self.batch_size],\n",
    "                                       dataB[idx * self.batch_size:(idx + 1) * self.batch_size]))\n",
    "                batch_images = [load_train_data(batch_file, args.load_size, args.fine_size) for batch_file in batch_files]\n",
    "                batch_images = np.array(batch_images).astype(np.float32)\n",
    "\n",
    "                # Update G network and record fake outputs\n",
    "                fake_A, fake_B, _, summary_str = self.sess.run(\n",
    "                    [self.fake_A, self.fake_B, self.g_optim, self.g_sum],\n",
    "                    feed_dict={self.real_data: batch_images, self.lr: lr})\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "                [fake_A, fake_B] = self.pool([fake_A, fake_B])\n",
    "\n",
    "                # Update D network\n",
    "                _, summary_str = self.sess.run(\n",
    "                    [self.d_optim, self.d_sum],\n",
    "                    feed_dict={self.real_data: batch_images,\n",
    "                               self.fake_A_sample: fake_A,\n",
    "                               self.fake_B_sample: fake_B,\n",
    "                               self.lr: lr})\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                counter += 1\n",
    "                print((\"Epoch: [%2d] [%4d/%4d] time: %4.4f\" % (\n",
    "                    epoch, idx, batch_idxs, time.time() - start_time)))\n",
    "\n",
    "                if np.mod(counter, args.print_freq) == 1:\n",
    "                    self.sample_model(args.sample_dir, epoch, idx)\n",
    "\n",
    "                if np.mod(counter, args.save_freq) == 2:\n",
    "                    self.save(args.checkpoint_dir, counter)\n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        model_name = \"cyclegan.model\"\n",
    "        model_dir = \"%s_%s\" % (self.dataset_dir, self.image_size)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,\n",
    "                        os.path.join(checkpoint_dir, model_name),\n",
    "                        global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        print(\" [*] Reading checkpoint...\")\n",
    "\n",
    "        model_dir = \"%s_%s\" % (self.dataset_dir, self.image_size)\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def sample_model(self, sample_dir, epoch, idx):\n",
    "        dataA = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/testA'))\n",
    "        dataB = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/testB'))\n",
    "        np.random.shuffle(dataA)\n",
    "        np.random.shuffle(dataB)\n",
    "        batch_files = list(zip(dataA[:self.batch_size], dataB[:self.batch_size]))\n",
    "        sample_images = [load_train_data(batch_file, is_testing=True) for batch_file in batch_files]\n",
    "        sample_images = np.array(sample_images).astype(np.float32)\n",
    "\n",
    "        fake_A, fake_B = self.sess.run(\n",
    "            [self.fake_A, self.fake_B],\n",
    "            feed_dict={self.real_data: sample_images}\n",
    "        )\n",
    "        save_images(fake_A, [self.batch_size, 1],\n",
    "                    './{}/A_{:02d}_{:04d}.jpg'.format(sample_dir, epoch, idx))\n",
    "        save_images(fake_B, [self.batch_size, 1],\n",
    "                    './{}/B_{:02d}_{:04d}.jpg'.format(sample_dir, epoch, idx))\n",
    "\n",
    "    def test(self, args):\n",
    "        \"\"\"Test cyclegan\"\"\"\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        self.sess.run(init_op)\n",
    "        if args.which_direction == 'AtoB':\n",
    "            sample_files = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/testA'))\n",
    "        elif args.which_direction == 'BtoA':\n",
    "            sample_files = glob('./datasets/{}/*.*'.format(self.dataset_dir + '/testB'))\n",
    "        else:\n",
    "            raise Exception('--which_direction must be AtoB or BtoA')\n",
    "\n",
    "        if self.load(args.checkpoint_dir):\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            print(\" [!] Load failed...\")\n",
    "\n",
    "        # write html for visual comparison\n",
    "        index_path = os.path.join(args.test_dir, '{0}_index.html'.format(args.which_direction))\n",
    "        index = open(index_path, \"w\")\n",
    "        index.write(\"<html><body><table><tr>\")\n",
    "        index.write(\"<th>name</th><th>input</th><th>output</th></tr>\")\n",
    "\n",
    "        out_var, in_var = (self.testB, self.test_A) if args.which_direction == 'AtoB' else (\n",
    "            self.testA, self.test_B)\n",
    "\n",
    "        for sample_file in sample_files:\n",
    "            print('Processing image: ' + sample_file)\n",
    "            sample_image = [load_test_data(sample_file, args.fine_size)]\n",
    "            sample_image = np.array(sample_image).astype(np.float32)\n",
    "            image_path = os.path.join(args.test_dir,\n",
    "                                      '{0}_{1}'.format(args.which_direction, os.path.basename(sample_file)))\n",
    "            fake_img = self.sess.run(out_var, feed_dict={in_var: sample_image})\n",
    "            save_images(fake_img, [1, 1], image_path)\n",
    "            index.write(\"<td>%s</td>\" % os.path.basename(image_path))\n",
    "            index.write(\"<td><img src='%s'></td>\" % (sample_file if os.path.isabs(sample_file) else (\n",
    "                '..' + os.path.sep + sample_file)))\n",
    "            index.write(\"<td><img src='%s'></td>\" % (image_path if os.path.isabs(image_path) else (\n",
    "                '..' + os.path.sep + image_path)))\n",
    "            index.write(\"</tr>\")\n",
    "        index.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
